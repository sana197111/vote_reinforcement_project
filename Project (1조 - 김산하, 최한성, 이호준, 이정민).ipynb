{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8lu2M87Ll6O"
      },
      "outputs": [],
      "source": [
        "#data_source_from https://en.wikipedia.org/wiki/United_States_Electoral_College\n",
        "\n",
        "Electoral_voters_per_state = {\n",
        "    \"California\" : 54,\n",
        "    \"Texas\" : 40,\n",
        "    \"Florida\" : 30,\n",
        "    \"New York\" : 28,\n",
        "    \"Illinois\" : 19, \n",
        "    \"Pennsylvania\" : 19,\n",
        "    \"Ohio\" : 17,\n",
        "    \"Georgia\" : 16,\n",
        "    \"North Carolina\" : 16,\n",
        "    \"Michigan\" : 15,\n",
        "    \"New Jersey\" : 14,\n",
        "    \"Virginia\" : 13,\n",
        "    \"Washington\" : 12,\n",
        "    \"Arizona\" : 11,\n",
        "    \"Indiana\" : 11,\n",
        "    \"Massachusetts\" : 11,\n",
        "    \"Tennessee\" : 11,\n",
        "    \"Colorado\" : 10,\n",
        "    \"Maryland\" : 10,\n",
        "    \"Minnesota\" : 10,\n",
        "    \"Missouri\" : 10,\n",
        "    \"Wisconsin\" : 10,\n",
        "    \"Alabama\" : 9,\n",
        "    \"South Carolina\" : 9,\n",
        "    \"Kentucky\" : 8,\n",
        "    \"Louisiana\" : 8,\n",
        "    \"Oregon\" : 8,\n",
        "    \"Connecticut\" : 7,\n",
        "    \"Oklahoma\" : 7,\n",
        "    \"Arkansas\" : 6,\n",
        "    \"Iowa\" : 6,\n",
        "    \"Kansas\" : 6,\n",
        "    \"Mississippi\" : 6,\n",
        "    \"Nevada\" : 6,\n",
        "    \"Utah\" : 6,\n",
        "    \"Nebraska\" : 5,\n",
        "    \"New Mexico\" : 5,\n",
        "    \"Hawaii\" : 4,\n",
        "    \"Idaho\" : 4,\n",
        "    \"Maine\" : 4,\n",
        "    \"Montana\" : 4,\n",
        "    \"New Hampshire\" : 4,\n",
        "    \"Rhode Island\" : 4,\n",
        "    \"West Virginia\" : 4,\n",
        "    \"Alaska\" : 3,\n",
        "    \"Delaware\" : 3,\n",
        "    \"District of Columbia\" : 3,\n",
        "    \"North Dakota\" : 3,\n",
        "    \"South Dakota\" : 3,\n",
        "    \"Vermont\" : 3,\n",
        "    \"Wyoming\" : 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIT-RVxkO4oJ",
        "outputId": "1c0661bb-ff4f-4a12-c2f3-453b74e9e4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ],
      "source": [
        "print(len(Electoral_voters_per_state.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfaYXh-zPjsx"
      },
      "outputs": [],
      "source": [
        "Initial_support_per_state = {\n",
        "    \"California\" : \"blue\",\n",
        "    \"Texas\" : \"red\",\n",
        "    \"Florida\" : \"swing\",\n",
        "    \"New York\" : \"blue\",\n",
        "    \"Illinois\" : \"blue\", \n",
        "    \"Pennsylvania\" : \"swing\",\n",
        "    \"Ohio\" : \"red\",\n",
        "    \"Georgia\" : \"swing\",\n",
        "    \"North Carolina\" : \"swing\",\n",
        "    \"Michigan\" : \"swing\",\n",
        "    \"New Jersey\" : \"blue\",\n",
        "    \"Virginia\" : \"swingy\",\n",
        "    \"Washington\" : \"bluye\",\n",
        "    \"Arizona\" : \"swing\",\n",
        "    \"Indiana\" : \"red\",\n",
        "    \"Massachusetts\" : \"blue\",\n",
        "    \"Tennessee\" : \"red\",\n",
        "    \"Colorado\" : \"blue\",\n",
        "    \"Maryland\" : \"blue\",\n",
        "    \"Minnesota\" : \"swing\",\n",
        "    \"Missouri\" : \"red\",\n",
        "    \"Wisconsin\" : \"swing\",\n",
        "    \"Alabama\" : \"red\",\n",
        "    \"South Carolina\" : \"red\",\n",
        "    \"Kentucky\" : \"red\",\n",
        "    \"Louisiana\" : \"red\",\n",
        "    \"Oregon\" : \"blue\",\n",
        "    \"Connecticut\" : \"blue\",\n",
        "    \"Oklahoma\" : \"red\",\n",
        "    \"Arkansas\" : \"red\",\n",
        "    \"Iowa\" : \"red\",\n",
        "    \"Kansas\" : \"red\",\n",
        "    \"Mississippi\" : \"red\",\n",
        "    \"Nevada\" : \"swing\",\n",
        "    \"Utah\" : \"red\",\n",
        "    \"Nebraska\" : \"red\",\n",
        "    \"New Mexico\" : \"swing\",\n",
        "    \"Hawaii\" : \"blue\",\n",
        "    \"Idaho\" : \"red\",\n",
        "    \"Maine\" : \"swing\",\n",
        "    \"Montana\" : \"red\",\n",
        "    \"New Hampshire\" : \"swing\",\n",
        "    \"Rhode Island\" : \"blue\",\n",
        "    \"West Virginia\" : \"red\",\n",
        "    \"Alaska\" : \"red\",\n",
        "    \"Delaware\" : \"blue\",\n",
        "    \"District of Columbia\" : \"blue\",\n",
        "    \"North Dakota\" : \"red\",\n",
        "    \"South Dakota\" : \"red\",\n",
        "    \"Vermont\" : \"blue\",\n",
        "    \"Wyoming\" : \"red\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpvTUJvSWk1U",
        "outputId": "886be381-bf39-47df-ed6c-55af047cb3c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'California': 'blue', 'Texas': 'red', 'Florida': 'swing', 'New York': 'blue', 'Illinois': 'blue', 'Pennsylvania': 'swing', 'Ohio': 'red', 'Georgia': 'swing', 'North Carolina': 'swing', 'Michigan': 'swing', 'New Jersey': 'blue', 'Virginia': 'swingy', 'Washington': 'bluye', 'Arizona': 'swing', 'Indiana': 'red', 'Massachusetts': 'blue', 'Tennessee': 'red', 'Colorado': 'blue', 'Maryland': 'blue', 'Minnesota': 'swing', 'Missouri': 'red', 'Wisconsin': 'swing', 'Alabama': 'red', 'South Carolina': 'red', 'Kentucky': 'red', 'Louisiana': 'red', 'Oregon': 'blue', 'Connecticut': 'blue', 'Oklahoma': 'red', 'Arkansas': 'red', 'Iowa': 'red', 'Kansas': 'red', 'Mississippi': 'red', 'Nevada': 'swing', 'Utah': 'red', 'Nebraska': 'red', 'New Mexico': 'swing', 'Hawaii': 'blue', 'Idaho': 'red', 'Maine': 'swing', 'Montana': 'red', 'New Hampshire': 'swing', 'Rhode Island': 'blue', 'West Virginia': 'red', 'Alaska': 'red', 'Delaware': 'blue', 'District of Columbia': 'blue', 'North Dakota': 'red', 'South Dakota': 'red', 'Vermont': 'blue', 'Wyoming': 'red'}\n"
          ]
        }
      ],
      "source": [
        "print(Initial_support_per_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diLwosZTWP4b",
        "outputId": "3fdd1ab1-19d4-43f5-ada6-ebd2030e6657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Delaware', 'Iowa', 'Utah', 'California', 'North Dakota', 'Kansas', 'Colorado', 'Minnesota', 'Massachusetts', 'Nebraska', 'Tennessee', 'Florida', 'Illinois', 'Louisiana', 'Vermont', 'Oklahoma', 'District of Columbia', 'New Jersey', 'South Dakota', 'Michigan', 'New Mexico', 'Idaho', 'South Carolina', 'Missouri', 'Montana', 'Connecticut', 'Pennsylvania', 'Oregon', 'Nevada', 'Mississippi', 'Alabama', 'Wisconsin', 'Georgia', 'Ohio', 'West Virginia', 'North Carolina', 'Wyoming', 'Kentucky', 'Maryland', 'Alaska', 'Rhode Island', 'New York', 'Arizona', 'Washington', 'Indiana', 'Texas', 'Virginia', 'Arkansas', 'Maine', 'New Hampshire', 'Hawaii']\n",
            "{'Delaware': 0, 'Iowa': 1, 'Utah': 2, 'California': 3, 'North Dakota': 4, 'Kansas': 5, 'Colorado': 6, 'Minnesota': 7, 'Massachusetts': 8, 'Nebraska': 9, 'Tennessee': 10, 'Florida': 11, 'Illinois': 12, 'Louisiana': 13, 'Vermont': 14, 'Oklahoma': 15, 'District of Columbia': 16, 'New Jersey': 17, 'South Dakota': 18, 'Michigan': 19, 'New Mexico': 20, 'Idaho': 21, 'South Carolina': 22, 'Missouri': 23, 'Montana': 24, 'Connecticut': 25, 'Pennsylvania': 26, 'Oregon': 27, 'Nevada': 28, 'Mississippi': 29, 'Alabama': 30, 'Wisconsin': 31, 'Georgia': 32, 'Ohio': 33, 'West Virginia': 34, 'North Carolina': 35, 'Wyoming': 36, 'Kentucky': 37, 'Maryland': 38, 'Alaska': 39, 'Rhode Island': 40, 'New York': 41, 'Arizona': 42, 'Washington': 43, 'Indiana': 44, 'Texas': 45, 'Virginia': 46, 'Arkansas': 47, 'Maine': 48, 'New Hampshire': 49, 'Hawaii': 50}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "a = list(Initial_support_per_state.keys())\n",
        "random.shuffle(a)\n",
        "print(a)\n",
        "state_number = dict()\n",
        "for i in range(len(a)):\n",
        "  state_number[a[i]] = i\n",
        "print(state_number)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McmTD4v9WInr"
      },
      "outputs": [],
      "source": [
        "state_number_dict = {'Alabama': 0,\n",
        "                'Wisconsin': 1,\n",
        "                'Wyoming': 2,\n",
        "                'Tennessee': 3,\n",
        "                'Iowa': 4,\n",
        "                'Maine': 5,\n",
        "                'Illinois': 6,\n",
        "                'Kentucky': 7,\n",
        "                'Montana': 8,\n",
        "                'Arizona': 9,\n",
        "                'Kansas': 10,\n",
        "                'Pennsylvania': 11,\n",
        "                'Utah': 12,\n",
        "                'Connecticut': 13,\n",
        "                'Nevada': 14,\n",
        "                'West Virginia': 15,\n",
        "                'Massachusetts': 16,\n",
        "                'Oklahoma': 17, \n",
        "                'Vermont': 18,\n",
        "                'Colorado': 19,\n",
        "                'District of Columbia': 20, \n",
        "                'Ohio': 21,\n",
        "                'Mississippi': 22, \n",
        "                'North Dakota': 23, \n",
        "                'California': 24,\n",
        "                'Idaho': 25, \n",
        "                'Louisiana': 26,\n",
        "                'New Hampshire': 27,\n",
        "                'North Carolina': 28,\n",
        "                'Nebraska': 29, \n",
        "                'Washington': 30, \n",
        "                'Missouri': 31, \n",
        "                'Delaware': 32, \n",
        "                'South Dakota': 33,\n",
        "                'Florida': 34,\n",
        "                'South Carolina': 35,\n",
        "                'Georgia': 36,\n",
        "                'Virginia': 37, \n",
        "                'Indiana': 38, \n",
        "                'Maryland': 39, \n",
        "                'Hawaii': 40, \n",
        "                'Arkansas': 41, \n",
        "                'New Jersey': 42, \n",
        "                'New York': 43,\n",
        "                'Texas': 44, \n",
        "                'New Mexico': 45,\n",
        "                'Oregon': 46, \n",
        "                'Rhode Island': 47, \n",
        "                'Michigan': 48, \n",
        "                'Alaska': 49, \n",
        "                'Minnesota': 50}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf_fTH4HUyqt"
      },
      "outputs": [],
      "source": [
        "#calculation is based on the statistics of https://edition.cnn.com/election/2020/results/president\n",
        "\n",
        "initial_red_superior_mean = 60\n",
        "initial_blue_superior_mean = 58\n",
        "initial_swing_point = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLwbeMESZNmG"
      },
      "outputs": [],
      "source": [
        "# Electoral_voters_per_state, Initial_support_per_state, state_number_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZyBNRvOz8Gi"
      },
      "outputs": [],
      "source": [
        "def make_init_state(Electoral_voters_per_state, Initial_support_per_state, state_number_dict):\n",
        "  Initial_state = np.zeros((51,3))\n",
        "  state_list = list(state_number_dict.keys())\n",
        "  for state in state_list:\n",
        "    Initial_state[state_number_dict[state]][0] = Electoral_voters_per_state[state]\n",
        "\n",
        "    red_luck = np.random.normal(0, 3)\n",
        "    blue_luck = np.random.normal(0, 3)\n",
        "\n",
        "    #Initial_state[state_number_dict[state_k]][1] means the current point of red at state_k\n",
        "    #Initial_state[state_number_dict[state_k]][2] means the current point of blue at state_k\n",
        "\n",
        "    if Initial_support_per_state[state] == 'swing':\n",
        "      Initial_state[state_number_dict[state]][1] = 50 +red_luck\n",
        "      Initial_state[state_number_dict[state]][2] = 50 +blue_luck\n",
        "\n",
        "    elif Initial_support_per_state[state] == 'red':\n",
        "\n",
        "      Initial_state[state_number_dict[state]][1] = initial_red_superior_mean +red_luck\n",
        "      Initial_state[state_number_dict[state]][2] = 100-initial_red_superior_mean +blue_luck\n",
        "\n",
        "    elif Initial_support_per_state[state] == 'blue':\n",
        "\n",
        "      Initial_state[state_number_dict[state]][1] = 100-initial_blue_superior_mean +red_luck\n",
        "      Initial_state[state_number_dict[state]][2] = initial_blue_superior_mean +blue_luck\n",
        "\n",
        "    else:\n",
        "      print(\"error_occured. check Initial_support_per_state\")\n",
        "    \n",
        "  return Initial_state\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SubOptimalActions:\n",
        "    def __init__(self, state, standard_point):\n",
        "        self.state = state\n",
        "        self.standard_point = standard_point\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.state = state\n",
        "        \n",
        "    def sub_optimal_action_red(self):\n",
        "        new_state = self.state.copy()\n",
        "        score_diff = new_state[:, 1] - new_state[:, 2]\n",
        "        weighted_diff = (1 / score_diff) * self.standard_point\n",
        "        optimal_state = np.argmax(weighted_diff * new_state[:, 0])\n",
        "\n",
        "        return np.array([optimal_state])\n",
        "\n",
        "    def sub_optimal_action_blue(self):\n",
        "        new_state = self.state.copy()\n",
        "        score_diff = new_state[:, 2] - new_state[:, 1]\n",
        "        weighted_diff = (1 / score_diff) * self.standard_point\n",
        "        optimal_state = np.argmax(weighted_diff * new_state[:, 0])\n",
        "\n",
        "        return np.array([optimal_state])\n"
      ],
      "metadata": {
        "id": "IDNe_M9XCroI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v1mT7iVZT_m",
        "outputId": "028af745-bb41-4d93-a240-2da7b9e29df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error_occured. check Initial_support_per_state\n",
            "error_occured. check Initial_support_per_state\n",
            "[[ 9.         59.25999362 38.38602128]\n",
            " [10.         45.43646461 47.51993915]\n",
            " [ 3.         62.66483514 38.912305  ]\n",
            " [11.         53.26631539 35.55699403]\n",
            " [ 6.         58.31848173 37.95100498]\n",
            " [ 4.         44.58139062 47.67719223]\n",
            " [19.         41.44138469 59.57563416]\n",
            " [ 8.         60.8904084  32.08179029]\n",
            " [ 4.         61.14245559 34.70488017]\n",
            " [11.         49.14637513 54.91412002]\n",
            " [ 6.         61.71945349 41.57275447]\n",
            " [19.         51.88998076 50.05572843]\n",
            " [ 6.         59.30891522 36.20860504]\n",
            " [ 7.         47.98659297 59.86707216]\n",
            " [ 6.         47.65110043 50.47056937]\n",
            " [ 4.         57.68488506 35.67057366]\n",
            " [11.         40.96723353 56.62523941]\n",
            " [ 7.         61.97531085 40.8400035 ]\n",
            " [ 3.         44.53184553 59.98557043]\n",
            " [10.         45.17247858 56.42523154]\n",
            " [ 3.         41.42639017 54.99620096]\n",
            " [17.         57.96729119 38.08407796]\n",
            " [ 6.         60.92117908 37.87356068]\n",
            " [ 3.         58.16221918 38.88121539]\n",
            " [54.         42.6953275  61.7642515 ]\n",
            " [ 4.         59.84955188 36.62227868]\n",
            " [ 8.         60.06265518 41.48749176]\n",
            " [ 4.         52.15784481 47.80885736]\n",
            " [16.         50.61255461 54.60899329]\n",
            " [ 5.         64.35309871 45.75688817]\n",
            " [12.          0.          0.        ]\n",
            " [10.         65.05276114 45.74925169]\n",
            " [ 3.         39.25234272 59.62153136]\n",
            " [ 3.         56.62596112 37.11486475]\n",
            " [30.         50.13456083 51.28790088]\n",
            " [ 9.         57.41222381 40.56694388]\n",
            " [16.         51.16805185 48.41805438]\n",
            " [13.          0.          0.        ]\n",
            " [11.         60.58151196 40.01479883]\n",
            " [10.         42.3730257  56.60858776]\n",
            " [ 4.         41.52488747 54.93260596]\n",
            " [ 6.         62.29238101 39.22706086]\n",
            " [14.         40.31991142 53.25764308]\n",
            " [28.         39.07843285 57.80466731]\n",
            " [40.         58.62292454 40.00398884]\n",
            " [ 5.         47.25255353 45.46427383]\n",
            " [ 8.         42.01191098 62.53802105]\n",
            " [ 4.         42.2384687  60.56043735]\n",
            " [15.         46.435867   50.4107274 ]\n",
            " [ 3.         52.46567216 36.32679964]\n",
            " [10.         51.17755994 48.9854369 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "Initial_state = make_init_state(Electoral_voters_per_state, Initial_support_per_state, state_number_dict)\n",
        "print(Initial_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd87gPuhd4mn",
        "outputId": "916e5423-8de8-46b9-cc17-c47b5f372069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "#!pip install gym==0.23.1\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-KGxQsv5WEM",
        "outputId": "1a2be002-88a8-4963-80e1-ed338e08eefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(51, 3)\n"
          ]
        }
      ],
      "source": [
        "print(Initial_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYNtfp9ji876"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Box, Discrete, MultiDiscrete\n",
        "import random\n",
        "\n",
        "class US_vote(Env):\n",
        "  def __init__(self,initial_state,total_step):\n",
        "    #set initial state, state[i][0] = The number of electoral voters at i'th state\n",
        "    #state[i][1] = The point of red at i'th state, state[i][2] = The point of blue at i'th state\n",
        "    self.init_state = initial_state.copy()\n",
        "\n",
        "    #set observation space\n",
        "    self.obs_space = Box(low=-1000, high=1000, shape=initial_state.shape)\n",
        "\n",
        "    #set action space\n",
        "    self.action_space = MultiDiscrete(np.array([initial_state.shape[0]]))\n",
        "\n",
        "    #set current state\n",
        "    self.state = initial_state.copy()\n",
        "\n",
        "    #set end condition\n",
        "    self.total_step = total_step * 2\n",
        "    self.current_step = 0\n",
        "\n",
        "    #set history\n",
        "    self.archive = []\n",
        "    self.current_episode_history = []\n",
        "\n",
        "    self.standard_point = 3\n",
        "\n",
        "    #variables for rendering\n",
        "    self.red_reward = 0\n",
        "    self.blue_reward = 0\n",
        "    self.action = None\n",
        "    self.agent = None\n",
        "    self.info = {\"current winner\" : self.current_winner()}\n",
        "    \n",
        "  def step(self,agent,action):\n",
        "    #agent = [red,blue], action = the number of visiting state\n",
        "\n",
        "    self.current_episode_history.append(self.state.copy())\n",
        "    self.current_step += 1\n",
        "\n",
        "    \n",
        "    luck = np.random.normal(0, 1)\n",
        "    step_point = self.standard_point + luck\n",
        "\n",
        "    prior_state = self.state.copy()\n",
        "\n",
        "\n",
        "    assert (agent == 'red' or agent == 'blue'), 'agent should be red or blue.'\n",
        "    assert 0<= action < self.state.shape[0], 'action should be 0<= action < {}'.format(self.state.shape[0])\n",
        "\n",
        "    if agent == 'red':\n",
        "      new_state = self.state.copy()\n",
        "      new_state[action,1] += step_point\n",
        "    elif agent == 'blue':\n",
        "      new_state = self.state.copy()\n",
        "      new_state[action,2] += step_point\n",
        "\n",
        "    self.state = new_state.copy()\n",
        "    red_reward = 0\n",
        "    blue_reward = 0\n",
        "\n",
        "    if (prior_state[action,1]-prior_state[action,2])*(new_state[action,1]-new_state[action,2]) < 0: #this case appears when point difference reversed\n",
        "      if new_state[action,1] > new_state[action,2]:\n",
        "          red_reward = self.state[action,0]\n",
        "          blue_reward = -1*self.state[action,0]\n",
        "      else:\n",
        "          red_reward = -1*self.state[action,0]\n",
        "          blue_reward = self.state[action,0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    done = False\n",
        "    if self.current_step >= self.total_step: # this happens when election campaign ended.\n",
        "      done = True\n",
        "      self.current_episode_history.append(self.state.copy())\n",
        "      self.archive.append(self.current_episode_history)\n",
        "      self.current_episode_history = []\n",
        "      winner = self.current_winner()\n",
        "      if winner == \"red\":\n",
        "        red_reward += 538\n",
        "        blue_reward -= 538\n",
        "      if winner == \"blue\":\n",
        "        red_reward -= 538\n",
        "        blue_reward += 538\n",
        "\n",
        "\n",
        "\n",
        "    info = {\"current winner\" : self.current_winner()}\n",
        "\n",
        "\n",
        "    self.action = action\n",
        "    self.agent = agent\n",
        "    self.red_reward = red_reward\n",
        "    self.blue_reward = blue_reward\n",
        "    self.info = info\n",
        "\n",
        "    return self.state.copy(), red_reward, blue_reward, done, info\n",
        "\n",
        "\n",
        "  def current_winner(self):\n",
        "\n",
        "      red_voters = 0\n",
        "      blue_voters = 0\n",
        "      for i in range(self.state.shape[0]):\n",
        "        if self.state[i,1] - self.state[i,2] > 0:\n",
        "          red_voters += self.state[i][0]\n",
        "        elif self.state[i,1] - self.state[i,2] < 0:\n",
        "          blue_voters += self.state[i,0]\n",
        "      \n",
        "      if red_voters > blue_voters:\n",
        "        return \"red\"\n",
        "      elif red_voters < blue_voters:\n",
        "        return \"blue\"\n",
        "      else:\n",
        "        return \"both, red and blue has same voters\"\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = self.init_state.copy()\n",
        "    self.current_step = 0\n",
        "    self.archive = []\n",
        "    self.current_episode_history = []\n",
        "    self.action = None\n",
        "    self.agent = None\n",
        "    self.red_reward = 0\n",
        "    self.blue_reward = 0\n",
        "    self.info = {\"current winner\" : self.current_winner()}\n",
        "\n",
        "    return self.state\n",
        "\n",
        "  def reset_with_save_archive(self):\n",
        "    self.state = self.init_state.copy()\n",
        "    self.current_step = 0\n",
        "    self.current_episode_history = []\n",
        "    self.action = None\n",
        "    self.agent = None\n",
        "    self.red_reward = 0\n",
        "    self.blue_reward = 0\n",
        "    self.info = {\"current winner\" : self.current_winner()}\n",
        "\n",
        "    return self.state\n",
        "\n",
        "\n",
        "  def render(self):\n",
        "      print(f\"Step : {self.current_step}\\n{self.agent} visited {self.action} state\")\n",
        "      print(f\"red_reward = {self.red_reward}, blue_reward = {self.blue_reward}\")\n",
        "      print(f\"current winner = {self.info['current winner']}\")\n",
        "      print(\"================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkXFLYgzsY3W",
        "outputId": "45fb8172-703d-47c1-c252-ba5e33f8b06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'env = US_vote(Initial_state,180)\\n\\ndone = False\\n\\nwhile not done:\\n    env.render()\\n    red_action = env.action_space.sample()\\n    state, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\\n    \\n    env.render()\\n    blue_action = env.action_space.sample()\\n    state, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "'''env = US_vote(Initial_state,180)\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    red_action = env.action_space.sample()\n",
        "    state, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "    \n",
        "    env.render()\n",
        "    blue_action = env.action_space.sample()\n",
        "    state, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''env = US_vote(Initial_state, 180)\n",
        "actions = SubOptimalActions(env.state, env.standard_point)\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    red_action = env.action_space.sample()\n",
        "    state, red_reward, blue_reward, done, info = env.step(\"red\", red_action)\n",
        "    \n",
        "    actions.set_state(state)  \n",
        "    \n",
        "    env.render()\n",
        "    blue_action = actions.sub_optimal_action_blue()  \n",
        "    state, red_reward, blue_reward, done, info = env.step(\"blue\", blue_action)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zal_aHTmWhuZ",
        "outputId": "935c9254-8124-4def-db52-4ae041f38b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'env = US_vote(Initial_state, 180)\\nactions = SubOptimalActions(env.state, env.standard_point)\\n\\ndone = False\\n\\nwhile not done:\\n    env.render()\\n    red_action = env.action_space.sample()\\n    state, red_reward, blue_reward, done, info = env.step(\"red\", red_action)\\n    \\n    actions.set_state(state)  \\n    \\n    env.render()\\n    blue_action = actions.sub_optimal_action_blue()  \\n    state, red_reward, blue_reward, done, info = env.step(\"blue\", blue_action)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''env = US_vote(Initial_state, 180)\n",
        "actions = SubOptimalActions(env.state, env.standard_point)\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    red_action = actions.sub_optimal_action_red()  \n",
        "    state, red_reward, blue_reward, done, info = env.step(\"red\", red_action)\n",
        "    \n",
        "    actions.set_state(state)  \n",
        "    \n",
        "    env.render()\n",
        "    blue_action = actions.sub_optimal_action_blue()  \n",
        "    state, red_reward, blue_reward, done, info = env.step(\"blue\", blue_action)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTNrngJ0LHLA",
        "outputId": "ce7e9e76-6565-46e0-b40e-f9fbe44b055a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'env = US_vote(Initial_state, 180)\\nactions = SubOptimalActions(env.state, env.standard_point)\\n\\ndone = False\\n\\nwhile not done:\\n    env.render()\\n    red_action = actions.sub_optimal_action_red()  \\n    state, red_reward, blue_reward, done, info = env.step(\"red\", red_action)\\n    \\n    actions.set_state(state)  \\n    \\n    env.render()\\n    blue_action = actions.sub_optimal_action_blue()  \\n    state, red_reward, blue_reward, done, info = env.step(\"blue\", blue_action)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbDFeh06lqEb",
        "outputId": "0b306114-cff3-4b8c-c810-81db405c42ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiDiscrete([51  2])\n"
          ]
        }
      ],
      "source": [
        "a = MultiDiscrete(np.array([51,2]))\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To do Deep Q-Network (DQN)\n",
        "\n",
        "source: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "https://yjs-program.tistory.com/173"
      ],
      "metadata": {
        "id": "tiuoQ4nbKvCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "import gym\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "EPISODES = 50\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "GAMMA = 0.8\n",
        "\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n",
        "\n",
        "def conv2d_size_out(size, kernel_size, stride):\n",
        "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "\n",
        "class redDQNAgent:\n",
        "  def __init__(self):\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(153,100),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(100,51)\n",
        "    )\n",
        "    self.optimizer = optim.Adam(self.model.parameters(),LR)\n",
        "    self.steps_done = 0\n",
        "    self.memory = deque(maxlen=50000)\n",
        "    self.reward_sum = 0.0\n",
        "\n",
        "  def memorize(self, situation, red_action, red_reward, blue_reward, next_situation):\n",
        "    situation=situation/800\n",
        "    situation=situation*[8, 1, 1]\n",
        "    situation=torch.Tensor(situation)\n",
        "    situation=situation.view(1,153)\n",
        "\n",
        "    red_reward = red_reward / 100\n",
        "    blue_reward = blue_reward / 100\n",
        "\n",
        "    red_action = torch.Tensor(red_action)\n",
        "    red_action = red_action.reshape(1)\n",
        "    \n",
        "    red_reward = torch.from_numpy(np.array(red_reward))\n",
        "    red_reward = red_reward.reshape(1)\n",
        "    blue_reward = torch.from_numpy(np.array(blue_reward))\n",
        "    blue_reward = blue_reward.reshape(1)\n",
        "\n",
        "    next_situation=next_situation/800\n",
        "    next_situation=next_situation*[8, 1, 1]\n",
        "    next_situation=torch.Tensor(next_situation)\n",
        "    next_situation=next_situation.view(1,153)\n",
        "\n",
        "    situation = situation.type(torch.FloatTensor)\n",
        "    red_action = red_action.type(torch.int64)\n",
        "    red_reward = red_reward.type(torch.FloatTensor)\n",
        "    blue_reward = blue_reward.type(torch.FloatTensor)\n",
        "    next_situation = next_situation.type(torch.FloatTensor)\n",
        "\n",
        "    self.memory.append((situation, red_action, red_reward, blue_reward, next_situation))\n",
        "\n",
        "  def learn(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    batch=random.sample(self.memory, BATCH_SIZE)\n",
        "    situations, red_actions, red_rewards, blue_rewards, next_situations = zip(*batch)\n",
        "\n",
        "    situations=torch.cat(situations)\n",
        "    \n",
        "\n",
        "    red_actions=torch.cat(red_actions)\n",
        "    red_actions = red_actions.reshape(-1,1)\n",
        "    \n",
        "    red_rewards=torch.cat(red_rewards)\n",
        "    blue_rewards=torch.cat(blue_rewards)\n",
        "    next_situations=torch.cat(next_situations)\n",
        "\n",
        "    current_q=self.model(situations).gather(1, red_actions)\n",
        "\n",
        "    max_next_q=self.model(next_situations).detach().max(1)[0]\n",
        "    expected_q=red_rewards+(GAMMA*max_next_q)\n",
        "\n",
        "    loss=F.mse_loss(current_q.squeeze(),expected_q)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def get_q_value(self, situation):\n",
        "    situation = situation / 800\n",
        "    situation = situation * [8, 1, 1]\n",
        "    situation = torch.Tensor(situation)\n",
        "    situation = situation.view(-1, 153)\n",
        "    q_values = self.model(situation)\n",
        "    return q_values\n",
        "\n",
        "  def get_action(self, situation, epsilon=0.1):\n",
        "    q_values = self.get_q_value(situation)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # 탐험(exploration)을 위해 랜덤한 행동 선택\n",
        "        #print(\"if executed: \")\n",
        "        action = random.randint(0, 50)\n",
        "        action = torch.from_numpy(np.array(action))\n",
        "\n",
        "    else:\n",
        "        # 탐색된 정보를 바탕으로 가장 높은 Q-value를 가진 행동 선택\n",
        "        action = q_values.argmax()\n",
        "        #print(\"else executed: \", action)\n",
        "  \n",
        "    return action\n",
        "\n",
        "class redDQNAgent_Deep:\n",
        "  def __init__(self):\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(153,110),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(110,80),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(80,51)\n",
        "    )\n",
        "    self.optimizer = optim.Adam(self.model.parameters(),LR)\n",
        "    self.steps_done = 0\n",
        "    self.memory = deque(maxlen=50000)\n",
        "    self.reward_sum = 0.0\n",
        "\n",
        "  def memorize(self, situation, red_action, red_reward, blue_reward, next_situation):\n",
        "    situation=situation/800\n",
        "    situation=situation*[8, 1, 1]\n",
        "    situation=torch.Tensor(situation)\n",
        "    situation=situation.view(1,153)\n",
        "\n",
        "    red_reward = red_reward / 100\n",
        "    blue_reward = blue_reward / 100\n",
        "\n",
        "    red_action = torch.Tensor(red_action)\n",
        "    red_action = red_action.reshape(1)\n",
        "    \n",
        "    red_reward = torch.from_numpy(np.array(red_reward))\n",
        "    red_reward = red_reward.reshape(1)\n",
        "    blue_reward = torch.from_numpy(np.array(blue_reward))\n",
        "    blue_reward = blue_reward.reshape(1)\n",
        "\n",
        "    next_situation=next_situation/800\n",
        "    next_situation=next_situation*[8, 1, 1]\n",
        "    next_situation=torch.Tensor(next_situation)\n",
        "    next_situation=next_situation.view(1,153)\n",
        "\n",
        "    situation = situation.type(torch.FloatTensor)\n",
        "    red_action = red_action.type(torch.int64)\n",
        "    red_reward = red_reward.type(torch.FloatTensor)\n",
        "    blue_reward = blue_reward.type(torch.FloatTensor)\n",
        "    next_situation = next_situation.type(torch.FloatTensor)\n",
        "\n",
        "    self.memory.append((situation, red_action, red_reward, blue_reward, next_situation))\n",
        "\n",
        "  def learn(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    batch=random.sample(self.memory, BATCH_SIZE)\n",
        "    situations, red_actions, red_rewards, blue_rewards, next_situations = zip(*batch)\n",
        "\n",
        "    situations=torch.cat(situations)\n",
        "    \n",
        "\n",
        "    red_actions=torch.cat(red_actions)\n",
        "    red_actions = red_actions.reshape(-1,1)\n",
        "    \n",
        "    red_rewards=torch.cat(red_rewards)\n",
        "    blue_rewards=torch.cat(blue_rewards)\n",
        "    next_situations=torch.cat(next_situations)\n",
        "\n",
        "    current_q=self.model(situations).gather(1, red_actions)\n",
        "\n",
        "    max_next_q=self.model(next_situations).detach().max(1)[0]\n",
        "    expected_q=red_rewards+(GAMMA*max_next_q)\n",
        "\n",
        "    loss=F.mse_loss(current_q.squeeze(),expected_q)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def get_q_value(self, situation):\n",
        "    situation = situation / 800\n",
        "    situation = situation * [8, 1, 1]\n",
        "    situation = torch.Tensor(situation)\n",
        "    situation = situation.view(-1, 153)\n",
        "    q_values = self.model(situation)\n",
        "    return q_values\n",
        "\n",
        "  def get_action(self, situation, epsilon=0.1):\n",
        "    q_values = self.get_q_value(situation)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # 탐험(exploration)을 위해 랜덤한 행동 선택\n",
        "        #print(\"if executed: \")\n",
        "        action = random.randint(0, 50)\n",
        "        action = torch.from_numpy(np.array(action))\n",
        "\n",
        "    else:\n",
        "        # 탐색된 정보를 바탕으로 가장 높은 Q-value를 가진 행동 선택\n",
        "        action = q_values.argmax()\n",
        "        #print(\"else executed: \", action)\n",
        "  \n",
        "    return action    \n",
        "\n",
        "class redDQNAgent_SGD:\n",
        "  def __init__(self):\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(153,100),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(100,51)\n",
        "    )\n",
        "    self.optimizer = optim.SGD(self.model.parameters(),LR)\n",
        "    self.steps_done = 0\n",
        "    self.memory = deque(maxlen=50000)\n",
        "    self.reward_sum = 0.0\n",
        "\n",
        "  def memorize(self, situation, red_action, red_reward, blue_reward, next_situation):\n",
        "    situation=situation/800\n",
        "    situation=situation*[8, 1, 1]\n",
        "    situation=torch.Tensor(situation)\n",
        "    situation=situation.view(1,153)\n",
        "\n",
        "    red_reward = red_reward / 100\n",
        "    blue_reward = blue_reward / 100\n",
        "\n",
        "    red_action = torch.Tensor(red_action)\n",
        "    red_action = red_action.reshape(1)\n",
        "    \n",
        "    red_reward = torch.from_numpy(np.array(red_reward))\n",
        "    red_reward = red_reward.reshape(1)\n",
        "    blue_reward = torch.from_numpy(np.array(blue_reward))\n",
        "    blue_reward = blue_reward.reshape(1)\n",
        "\n",
        "    next_situation=next_situation/800\n",
        "    next_situation=next_situation*[8, 1, 1]\n",
        "    next_situation=torch.Tensor(next_situation)\n",
        "    next_situation=next_situation.view(1,153)\n",
        "\n",
        "    situation = situation.type(torch.FloatTensor)\n",
        "    red_action = red_action.type(torch.int64)\n",
        "    red_reward = red_reward.type(torch.FloatTensor)\n",
        "    blue_reward = blue_reward.type(torch.FloatTensor)\n",
        "    next_situation = next_situation.type(torch.FloatTensor)\n",
        "\n",
        "    self.memory.append((situation, red_action, red_reward, blue_reward, next_situation))\n",
        "\n",
        "  def learn(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    batch=random.sample(self.memory, BATCH_SIZE)\n",
        "    situations, red_actions, red_rewards, blue_rewards, next_situations = zip(*batch)\n",
        "\n",
        "    situations=torch.cat(situations)\n",
        "    \n",
        "\n",
        "    red_actions=torch.cat(red_actions)\n",
        "    red_actions = red_actions.reshape(-1,1)\n",
        "    \n",
        "    red_rewards=torch.cat(red_rewards)\n",
        "    blue_rewards=torch.cat(blue_rewards)\n",
        "    next_situations=torch.cat(next_situations)\n",
        "\n",
        "    current_q=self.model(situations).gather(1, red_actions)\n",
        "\n",
        "    max_next_q=self.model(next_situations).detach().max(1)[0]\n",
        "    expected_q=red_rewards+(GAMMA*max_next_q)\n",
        "\n",
        "    loss=F.mse_loss(current_q.squeeze(),expected_q)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def get_q_value(self, situation):\n",
        "    situation = situation / 800\n",
        "    situation = situation * [8, 1, 1]\n",
        "    situation = torch.Tensor(situation)\n",
        "    situation = situation.view(-1, 153)\n",
        "    q_values = self.model(situation)\n",
        "    return q_values\n",
        "\n",
        "  def get_action(self, situation, epsilon=0.1):\n",
        "    q_values = self.get_q_value(situation)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # 탐험(exploration)을 위해 랜덤한 행동 선택\n",
        "        #print(\"if executed: \")\n",
        "        action = random.randint(0, 50)\n",
        "        action = torch.from_numpy(np.array(action))\n",
        "\n",
        "    else:\n",
        "        # 탐색된 정보를 바탕으로 가장 높은 Q-value를 가진 행동 선택\n",
        "        action = q_values.argmax()\n",
        "        #print(\"else executed: \", action)\n",
        "  \n",
        "    return action\n",
        "\n",
        "class blueDQNAgent:\n",
        "  def __init__(self):\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(153,100),\n",
        "\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(100,51)\n",
        "    )\n",
        "    self.optimizer = optim.Adam(self.model.parameters(),LR)\n",
        "    self.steps_done = 0\n",
        "    self.memory = deque(maxlen=50000)\n",
        "    self.reward_sum = 0.0\n",
        "\n",
        "  def memorize(self, situation, blue_action, red_reward, blue_reward, next_situation):\n",
        "    situation=situation/800\n",
        "    situation=situation*[8, 1, 1]\n",
        "    situation=torch.Tensor(situation)\n",
        "    situation=situation.view(1,153)\n",
        "\n",
        "    red_reward = red_reward / 100\n",
        "    blue_reward = blue_reward / 100\n",
        "\n",
        "    blue_action = torch.Tensor(blue_action)\n",
        "    blue_action = blue_action.reshape(1)\n",
        "    \n",
        "    red_reward = torch.from_numpy(np.array(red_reward))\n",
        "    red_reward = red_reward.reshape(1)\n",
        "    blue_reward = torch.from_numpy(np.array(blue_reward))\n",
        "    blue_reward = blue_reward.reshape(1)\n",
        "\n",
        "    next_situation=next_situation/800\n",
        "    next_situation=next_situation*[8, 1, 1]\n",
        "    next_situation=torch.Tensor(next_situation)\n",
        "    next_situation=next_situation.view(1,153)\n",
        "\n",
        "    situation = situation.type(torch.FloatTensor)\n",
        "    blue_action = blue_action.type(torch.int64)\n",
        "    red_reward = red_reward.type(torch.FloatTensor)\n",
        "    blue_reward = blue_reward.type(torch.FloatTensor)\n",
        "    next_situation = next_situation.type(torch.FloatTensor)\n",
        "\n",
        "    self.memory.append((situation, blue_action, red_reward, blue_reward, next_situation))\n",
        "\n",
        "  def learn(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    batch=random.sample(self.memory, BATCH_SIZE)\n",
        "    situations, blue_actions, red_rewards, blue_rewards, next_situations = zip(*batch)\n",
        "\n",
        "    situations=torch.cat(situations)\n",
        "    \n",
        "\n",
        "    blue_actions=torch.cat(blue_actions)\n",
        "    blue_actions = blue_actions.reshape(-1,1)\n",
        "    \n",
        "    red_rewards=torch.cat(red_rewards)\n",
        "    blue_rewards=torch.cat(blue_rewards)\n",
        "    next_situations=torch.cat(next_situations)\n",
        "\n",
        "    current_q=self.model(situations).gather(1, blue_actions)\n",
        "    max_next_q=self.model(next_situations).detach().max(1)[0]\n",
        "    expected_q=blue_rewards+(GAMMA*max_next_q)\n",
        "\n",
        "    loss=F.mse_loss(current_q.squeeze(),expected_q)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def get_q_value(self, situation):\n",
        "    situation = situation / 800\n",
        "    situation = situation * [8, 1, 1]\n",
        "    situation = torch.Tensor(situation)\n",
        "    situation = situation.view(-1, 153)\n",
        "    q_values = self.model(situation)\n",
        "    return q_values\n",
        "\n",
        "  def get_action(self, situation, epsilon=0.1):\n",
        "    q_values = self.get_q_value(situation)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # 탐험(exploration)을 위해 랜덤한 행동 선택\n",
        "        #print(\"if executed: \")\n",
        "        action = random.randint(0, 50)\n",
        "        action = torch.from_numpy(np.array(action))\n",
        "\n",
        "    else:\n",
        "        # 탐색된 정보를 바탕으로 가장 높은 Q-value를 가진 행동 선택\n",
        "        action = q_values.argmax()\n",
        "        #print(\"else executed: \", action)\n",
        "  \n",
        "    return action\n",
        "\n"
      ],
      "metadata": {
        "id": "bL2ZkNa9RVVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b0d961-718c-4e02-9d7c-170c919a999f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tOBlcT_LmEOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def case_00():  \n",
        "  # Case.1: R: Sub / B: Sub\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_red()  \n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_blue()  \n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"Sub-optimal vs Sub-optimal\")\n",
        "  plt.legend(['Red: Sub-optimal', 'Blue: Sub-optimal'])\n",
        "  plt.show()\n",
        "\n",
        "def case_01():  \n",
        "  # Case.1: R: Random / B: Sub\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_red()  #env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      #red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = env.action_space.sample() #SubOptimalActions(situation, env.standard_point).sub_optimal_action_blue()  \n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      #blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"Random vs Sub-optimal\")\n",
        "  plt.legend(['Red: Random', 'Blue: Sub-optimal'])\n",
        "  plt.show()\n",
        "\n",
        "def case_0():  \n",
        "  # Case.1: R: Random / B: Random\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  red_cnt = 0\n",
        "  blue_cnt = 0\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    if red_agent.reward_sum > blue_agent.reward_sum:\n",
        "      red_cnt += 1\n",
        "    else:\n",
        "      blue_cnt +=1\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(red_cnt, blue_cnt)\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"Random vs Random\")\n",
        "  plt.legend(['Red: Random ', 'Blue: Random'])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_1():  \n",
        "  # Case.1: R: Q / B: Random\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DQN vs Random\")\n",
        "  plt.legend(['Red: DQN', 'Blue: Random'])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_2():\n",
        "  # Case.2: R: Q / B: Sub_optimal\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_blue()  \n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DQN vs Sub_optimal\")\n",
        "  plt.legend(['Red: DQN', 'Blue: Sub_optimal'])\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_3():\n",
        "  # Case.3: R: Random / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  red_cnt = 0\n",
        "  blue_cnt = 0\n",
        "  #epsilon = 0.99\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    #epsilon = epsilon*0.97\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    if red_agent.reward_sum > blue_agent.reward_sum:\n",
        "      red_cnt += 1\n",
        "    else:\n",
        "      blue_cnt +=1\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "  print(red_cnt, blue_cnt)\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"Random vs DQN\")\n",
        "  plt.legend(['Red: Random', 'Blue: DQN'])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_3_1():\n",
        "  # Case.3_1: R: Random / B: Q #learn based on suboptimal action\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  red_cnt = 0\n",
        "  blue_cnt = 0\n",
        "  #epsilon = 0.99\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    #epsilon = epsilon*0.97\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_blue()  \n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "\n",
        "  for e in range(1,20):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    #epsilon = epsilon*0.97\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    if red_agent.reward_sum > blue_agent.reward_sum:\n",
        "      red_cnt += 1\n",
        "    else:\n",
        "      blue_cnt +=1\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "  print(red_cnt, blue_cnt)\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"Random vs DQN\")\n",
        "  plt.legend(['Red: Random', 'Blue: DQN'])\n",
        "  plt.show()\n",
        "\n",
        "def case_4():\n",
        "  # Case.4: R: Sub_optimal / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      \n",
        "      red_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_red()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel('# of episodes')\n",
        "  plt.ylabel('Sum of rewards')\n",
        "  plt.title('Sub_optimal vs DQN')\n",
        "  plt.legend(['Red:Sub_optimal', 'Blue: DQN'])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_5():  \n",
        "  # Case.1: R: Q / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DQN vs DQN\")\n",
        "  plt.legend(['Red: DQN', 'Blue: DQN'])\n",
        "  plt.show()\n",
        "\n",
        "def case_6():  \n",
        "  # Case.1: R: Q_SGD / B: Random\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent_SGD()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DQN vs Random\")\n",
        "  plt.legend(['Red: DQN', 'Blue: Random'])\n",
        "  plt.show()\n",
        "\n",
        "def case_7():  \n",
        "  # Case.1: R: Q / B: Random\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent_Deep()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DQN vs Random\")\n",
        "  plt.legend(['Red: DQN', 'Blue: Random'])\n",
        "  plt.show()\n",
        "\n",
        "#case_00()\n",
        "#case_01()\n",
        "case_0()\n",
        "#case_1()\n",
        "#case_2()\n",
        "#case_3()\n",
        "#case_4()\n",
        "#case_5()\n",
        "#case_6()\n",
        "#case_7()\n",
        "#case_3_1()\n"
      ],
      "metadata": {
        "id": "XVv0uRxJLWJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "273765e5-6c4e-4f3e-f8c3-34e2f568c992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 81\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 82\n",
            "blue visited [11] state\n",
            "red_reward = [-19.], blue_reward = [19.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 83\n",
            "red visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 84\n",
            "blue visited [48] state\n",
            "red_reward = [-15.], blue_reward = [15.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 85\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 86\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 87\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 88\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 89\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 90\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 91\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 92\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 93\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 94\n",
            "blue visited [50] state\n",
            "red_reward = [-10.], blue_reward = [10.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 95\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 96\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 97\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 98\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 99\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 100\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 101\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 102\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 103\n",
            "red visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 104\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 105\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 106\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 107\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 108\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 109\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 110\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 111\n",
            "red visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 112\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 113\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 114\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 115\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 116\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 117\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 118\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 119\n",
            "red visited [14] state\n",
            "red_reward = [6.], blue_reward = [-6.]\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 120\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 121\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 122\n",
            "blue visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 123\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 124\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 125\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 126\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 127\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 128\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 129\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 130\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 131\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 132\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 133\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 134\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 135\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 136\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 137\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 138\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 139\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 140\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 141\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 142\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 143\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 144\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 145\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 146\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 147\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 148\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 149\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 150\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 151\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 152\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 153\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 154\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 155\n",
            "red visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 156\n",
            "blue visited [14] state\n",
            "red_reward = [-6.], blue_reward = [6.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 157\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 158\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 159\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 160\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 161\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 162\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 163\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 164\n",
            "blue visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 165\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 166\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 167\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 168\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 169\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 170\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 171\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 172\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 173\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 174\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 175\n",
            "red visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 176\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 177\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 178\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 179\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 180\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 181\n",
            "red visited [30] state\n",
            "red_reward = [12.], blue_reward = [-12.]\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 182\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 183\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 184\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 185\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 186\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 187\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 188\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 189\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 190\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 191\n",
            "red visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 192\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 193\n",
            "red visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 194\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 195\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 196\n",
            "blue visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 197\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 198\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = both, red and blue has same voters\n",
            "================================================================\n",
            "Step : 199\n",
            "red visited [14] state\n",
            "red_reward = [6.], blue_reward = [-6.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 200\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 201\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 202\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 203\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 204\n",
            "blue visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 205\n",
            "red visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 206\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 207\n",
            "red visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 208\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 209\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 210\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 211\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 212\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 213\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 214\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 215\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 216\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 217\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 218\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 219\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 220\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 221\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 222\n",
            "blue visited [30] state\n",
            "red_reward = [-12.], blue_reward = [12.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 223\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 224\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 225\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 226\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 227\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 228\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 229\n",
            "red visited [28] state\n",
            "red_reward = [16.], blue_reward = [-16.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 230\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 231\n",
            "red visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 232\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 233\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 234\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 235\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 236\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 237\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 238\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 239\n",
            "red visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 240\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 241\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 242\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 243\n",
            "red visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 244\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 245\n",
            "red visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 246\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 247\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 248\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 249\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 250\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 251\n",
            "red visited [50] state\n",
            "red_reward = [10.], blue_reward = [-10.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 252\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 253\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 254\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 255\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 256\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 257\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 258\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 259\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 260\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 261\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 262\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 263\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 264\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 265\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 266\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 267\n",
            "red visited [48] state\n",
            "red_reward = [15.], blue_reward = [-15.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 268\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 269\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 270\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 271\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 272\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 273\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 274\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 275\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 276\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 277\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 278\n",
            "blue visited [28] state\n",
            "red_reward = [-16.], blue_reward = [16.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 279\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 280\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 281\n",
            "red visited [11] state\n",
            "red_reward = [19.], blue_reward = [-19.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 282\n",
            "blue visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 283\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 284\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 285\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 286\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 287\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 288\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 289\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 290\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 291\n",
            "red visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 292\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 293\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 294\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 295\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 296\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 297\n",
            "red visited [5] state\n",
            "red_reward = [4.], blue_reward = [-4.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 298\n",
            "blue visited [5] state\n",
            "red_reward = [-4.], blue_reward = [4.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 299\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 300\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 301\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 302\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 303\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 304\n",
            "blue visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 305\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 306\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 307\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 308\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 309\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 310\n",
            "blue visited [14] state\n",
            "red_reward = [-6.], blue_reward = [6.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 311\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 312\n",
            "blue visited [50] state\n",
            "red_reward = [-10.], blue_reward = [10.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 313\n",
            "red visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 314\n",
            "blue visited [7] state\n",
            "red_reward = [-8.], blue_reward = [8.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 315\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 316\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 317\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 318\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 319\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 320\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 321\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 322\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 323\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 324\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 325\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 326\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 327\n",
            "red visited [7] state\n",
            "red_reward = [8.], blue_reward = [-8.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 328\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 329\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 330\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 331\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 332\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 333\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 334\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 335\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 336\n",
            "blue visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 337\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 338\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 339\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 340\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 341\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 342\n",
            "blue visited [7] state\n",
            "red_reward = [-8.], blue_reward = [8.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 343\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 344\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 345\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 346\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 347\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 348\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 349\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 350\n",
            "blue visited [11] state\n",
            "red_reward = [-19.], blue_reward = [19.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 351\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 352\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 353\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 354\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 355\n",
            "red visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 356\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 357\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 358\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 359\n",
            "red visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 0\n",
            "None visited None state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 1\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 2\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 3\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 4\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 5\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 6\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 7\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 8\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 9\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 10\n",
            "blue visited [28] state\n",
            "red_reward = [-16.], blue_reward = [16.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 11\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 12\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 13\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 14\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 15\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 16\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 17\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 18\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 19\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 20\n",
            "blue visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 21\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 22\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 23\n",
            "red visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 24\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 25\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 26\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 27\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 28\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 29\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 30\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 31\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 32\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 33\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 34\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 35\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 36\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 37\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 38\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 39\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 40\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 41\n",
            "red visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 42\n",
            "blue visited [27] state\n",
            "red_reward = [-4.], blue_reward = [4.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 43\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 44\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 45\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 46\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 47\n",
            "red visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 48\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 49\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 50\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 51\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 52\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 53\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 54\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 55\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 56\n",
            "blue visited [11] state\n",
            "red_reward = [-19.], blue_reward = [19.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 57\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 58\n",
            "blue visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 59\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 60\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 61\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 62\n",
            "blue visited [30] state\n",
            "red_reward = [-12.], blue_reward = [12.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 63\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 64\n",
            "blue visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 65\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 66\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 67\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 68\n",
            "blue visited [34] state\n",
            "red_reward = [-30.], blue_reward = [30.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 69\n",
            "red visited [14] state\n",
            "red_reward = [6.], blue_reward = [-6.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 70\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 71\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 72\n",
            "blue visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 73\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 74\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 75\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 76\n",
            "blue visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 77\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 78\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 79\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 80\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 81\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 82\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 83\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 84\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 85\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 86\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 87\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 88\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 89\n",
            "red visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 90\n",
            "blue visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 91\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 92\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 93\n",
            "red visited [27] state\n",
            "red_reward = [4.], blue_reward = [-4.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 94\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 95\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 96\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 97\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 98\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 99\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 100\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 101\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 102\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 103\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 104\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 105\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 106\n",
            "blue visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 107\n",
            "red visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 108\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 109\n",
            "red visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 110\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 111\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 112\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 113\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 114\n",
            "blue visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 115\n",
            "red visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 116\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 117\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 118\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 119\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 120\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 121\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 122\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 123\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 124\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 125\n",
            "red visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 126\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 127\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 128\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 129\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 130\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 131\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 132\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 133\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 134\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 135\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 136\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 137\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 138\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 139\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 140\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 141\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 142\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 143\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 144\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 145\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 146\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 147\n",
            "red visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 148\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 149\n",
            "red visited [30] state\n",
            "red_reward = [12.], blue_reward = [-12.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 150\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 151\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 152\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 153\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 154\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 155\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 156\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 157\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 158\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 159\n",
            "red visited [46] state\n",
            "red_reward = [8.], blue_reward = [-8.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 160\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 161\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 162\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 163\n",
            "red visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 164\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 165\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 166\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 167\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 168\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 169\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 170\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 171\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 172\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 173\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 174\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 175\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 176\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 177\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 178\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 179\n",
            "red visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 180\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 181\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 182\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 183\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 184\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 185\n",
            "red visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 186\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 187\n",
            "red visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 188\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 189\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 190\n",
            "blue visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 191\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 192\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 193\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 194\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 195\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 196\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 197\n",
            "red visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 198\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 199\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 200\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 201\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 202\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 203\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 204\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 205\n",
            "red visited [43] state\n",
            "red_reward = [28.], blue_reward = [-28.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 206\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 207\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 208\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 209\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 210\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 211\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 212\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 213\n",
            "red visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 214\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 215\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 216\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 217\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 218\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 219\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 220\n",
            "blue visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 221\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 222\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 223\n",
            "red visited [11] state\n",
            "red_reward = [19.], blue_reward = [-19.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 224\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 225\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 226\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 227\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 228\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 229\n",
            "red visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 230\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 231\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 232\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 233\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 234\n",
            "blue visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 235\n",
            "red visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 236\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 237\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 238\n",
            "blue visited [43] state\n",
            "red_reward = [-28.], blue_reward = [28.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 239\n",
            "red visited [43] state\n",
            "red_reward = [28.], blue_reward = [-28.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 240\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 241\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 242\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 243\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 244\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 245\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 246\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 247\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 248\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 249\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 250\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 251\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 252\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 253\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 254\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 255\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 256\n",
            "blue visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 257\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 258\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 259\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 260\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 261\n",
            "red visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 262\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 263\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 264\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 265\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 266\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 267\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 268\n",
            "blue visited [43] state\n",
            "red_reward = [-28.], blue_reward = [28.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 269\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 270\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 271\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 272\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 273\n",
            "red visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 274\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 275\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 276\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 277\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 278\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 279\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 280\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 281\n",
            "red visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 282\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 283\n",
            "red visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 284\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 285\n",
            "red visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 286\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 287\n",
            "red visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 288\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 289\n",
            "red visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 290\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 291\n",
            "red visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 292\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 293\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 294\n",
            "blue visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 295\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 296\n",
            "blue visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 297\n",
            "red visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 298\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 299\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 300\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 301\n",
            "red visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 302\n",
            "blue visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 303\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 304\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 305\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 306\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 307\n",
            "red visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 308\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 309\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 310\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 311\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 312\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 313\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 314\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 315\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 316\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 317\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 318\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 319\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 320\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 321\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 322\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 323\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 324\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 325\n",
            "red visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 326\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 327\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 328\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 329\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 330\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 331\n",
            "red visited [45] state\n",
            "red_reward = [5.], blue_reward = [-5.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 332\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 333\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 334\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 335\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 336\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 337\n",
            "red visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 338\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 339\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 340\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 341\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 342\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 343\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 344\n",
            "blue visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 345\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 346\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 347\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 348\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 349\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 350\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 351\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 352\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 353\n",
            "red visited [39] state\n",
            "red_reward = [10.], blue_reward = [-10.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 354\n",
            "blue visited [27] state\n",
            "red_reward = [-4.], blue_reward = [4.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 355\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 356\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 357\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 358\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 359\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 0\n",
            "None visited None state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 1\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 2\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 3\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 4\n",
            "blue visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 5\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 6\n",
            "blue visited [28] state\n",
            "red_reward = [-16.], blue_reward = [16.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 7\n",
            "red visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 8\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 9\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 10\n",
            "blue visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 11\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 12\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 13\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 14\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 15\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 16\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 17\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 18\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 19\n",
            "red visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 20\n",
            "blue visited [27] state\n",
            "red_reward = [-4.], blue_reward = [4.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 21\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 22\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 23\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 24\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 25\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 26\n",
            "blue visited [34] state\n",
            "red_reward = [-30.], blue_reward = [30.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 27\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 28\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 29\n",
            "red visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 30\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 31\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 32\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 33\n",
            "red visited [27] state\n",
            "red_reward = [4.], blue_reward = [-4.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 34\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 35\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 36\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 37\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 38\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 39\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 40\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 41\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 42\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 43\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 44\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 45\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 46\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 47\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 48\n",
            "blue visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 49\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 50\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 51\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 52\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 53\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 54\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 55\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 56\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 57\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 58\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 59\n",
            "red visited [5] state\n",
            "red_reward = [4.], blue_reward = [-4.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 60\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 61\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 62\n",
            "blue visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 63\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 64\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 65\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 66\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 67\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 68\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 69\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 70\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 71\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 72\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 73\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 74\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 75\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 76\n",
            "blue visited [11] state\n",
            "red_reward = [-19.], blue_reward = [19.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 77\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 78\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 79\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 80\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 81\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 82\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 83\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 84\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 85\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 86\n",
            "blue visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 87\n",
            "red visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 88\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 89\n",
            "red visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 90\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 91\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 92\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 93\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 94\n",
            "blue visited [27] state\n",
            "red_reward = [-4.], blue_reward = [4.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 95\n",
            "red visited [37] state\n",
            "red_reward = [13.], blue_reward = [-13.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 96\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 97\n",
            "red visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 98\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 99\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 100\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 101\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 102\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 103\n",
            "red visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 104\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 105\n",
            "red visited [14] state\n",
            "red_reward = [6.], blue_reward = [-6.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 106\n",
            "blue visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 107\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 108\n",
            "blue visited [37] state\n",
            "red_reward = [-13.], blue_reward = [13.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 109\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 110\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 111\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 112\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 113\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 114\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 115\n",
            "red visited [28] state\n",
            "red_reward = [16.], blue_reward = [-16.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 116\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 117\n",
            "red visited [45] state\n",
            "red_reward = [5.], blue_reward = [-5.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 118\n",
            "blue visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 119\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 120\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 121\n",
            "red visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 122\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 123\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 124\n",
            "blue visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 125\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 126\n",
            "blue visited [45] state\n",
            "red_reward = [-5.], blue_reward = [5.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 127\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 128\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 129\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 130\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 131\n",
            "red visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 132\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 133\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 134\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 135\n",
            "red visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 136\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 137\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 138\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 139\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 140\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 141\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 142\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 143\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 144\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 145\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 146\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 147\n",
            "red visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 148\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 149\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 150\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 151\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 152\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 153\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 154\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 155\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 156\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 157\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 158\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 159\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 160\n",
            "blue visited [39] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 161\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 162\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 163\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 164\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 165\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 166\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 167\n",
            "red visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 168\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 169\n",
            "red visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 170\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 171\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 172\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 173\n",
            "red visited [44] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 174\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 175\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 176\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 177\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 178\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 179\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 180\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 181\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 182\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 183\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 184\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 185\n",
            "red visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 186\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 187\n",
            "red visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 188\n",
            "blue visited [9] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 189\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 190\n",
            "blue visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 191\n",
            "red visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 192\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 193\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 194\n",
            "blue visited [29] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 195\n",
            "red visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 196\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 197\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 198\n",
            "blue visited [34] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 199\n",
            "red visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 200\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 201\n",
            "red visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 202\n",
            "blue visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 203\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 204\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 205\n",
            "red visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 206\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 207\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 208\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 209\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 210\n",
            "blue visited [20] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 211\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 212\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 213\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 214\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 215\n",
            "red visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 216\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 217\n",
            "red visited [11] state\n",
            "red_reward = [19.], blue_reward = [-19.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 218\n",
            "blue visited [14] state\n",
            "red_reward = [-6.], blue_reward = [6.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 219\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 220\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 221\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 222\n",
            "blue visited [50] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 223\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 224\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 225\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 226\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 227\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 228\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 229\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 230\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 231\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 232\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 233\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 234\n",
            "blue visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 235\n",
            "red visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 236\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 237\n",
            "red visited [14] state\n",
            "red_reward = [6.], blue_reward = [-6.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 238\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 239\n",
            "red visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 240\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 241\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 242\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 243\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 244\n",
            "blue visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 245\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 246\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 247\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 248\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 249\n",
            "red visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 250\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 251\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 252\n",
            "blue visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 253\n",
            "red visited [43] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 254\n",
            "blue visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 255\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 256\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 257\n",
            "red visited [18] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 258\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 259\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 260\n",
            "blue visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 261\n",
            "red visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 262\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 263\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 264\n",
            "blue visited [10] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 265\n",
            "red visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 266\n",
            "blue visited [2] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 267\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 268\n",
            "blue visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 269\n",
            "red visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 270\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 271\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 272\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 273\n",
            "red visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 274\n",
            "blue visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 275\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 276\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 277\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 278\n",
            "blue visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 279\n",
            "red visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 280\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 281\n",
            "red visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 282\n",
            "blue visited [24] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 283\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 284\n",
            "blue visited [25] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 285\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 286\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 287\n",
            "red visited [24] state\n",
            "red_reward = [54.], blue_reward = [-54.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 288\n",
            "blue visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 289\n",
            "red visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 290\n",
            "blue visited [17] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 291\n",
            "red visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 292\n",
            "blue visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 293\n",
            "red visited [48] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 294\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 295\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 296\n",
            "blue visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 297\n",
            "red visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 298\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 299\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 300\n",
            "blue visited [35] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 301\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 302\n",
            "blue visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 303\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 304\n",
            "blue visited [5] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 305\n",
            "red visited [47] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 306\n",
            "blue visited [45] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 307\n",
            "red visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 308\n",
            "blue visited [13] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 309\n",
            "red visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 310\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 311\n",
            "red visited [27] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 312\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 313\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 314\n",
            "blue visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 315\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 316\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 317\n",
            "red visited [46] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 318\n",
            "blue visited [41] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 319\n",
            "red visited [22] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 320\n",
            "blue visited [31] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 321\n",
            "red visited [32] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 322\n",
            "blue visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 323\n",
            "red visited [19] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 324\n",
            "blue visited [30] state\n",
            "red_reward = [-12.], blue_reward = [12.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 325\n",
            "red visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 326\n",
            "blue visited [24] state\n",
            "red_reward = [-54.], blue_reward = [54.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 327\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 328\n",
            "blue visited [21] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 329\n",
            "red visited [7] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 330\n",
            "blue visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 331\n",
            "red visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 332\n",
            "blue visited [6] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 333\n",
            "red visited [15] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 334\n",
            "blue visited [14] state\n",
            "red_reward = [-6.], blue_reward = [6.]\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 335\n",
            "red visited [8] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 336\n",
            "blue visited [37] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 337\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 338\n",
            "blue visited [36] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 339\n",
            "red visited [38] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 340\n",
            "blue visited [33] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 341\n",
            "red visited [40] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 342\n",
            "blue visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 343\n",
            "red visited [4] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 344\n",
            "blue visited [16] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 345\n",
            "red visited [3] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = red\n",
            "================================================================\n",
            "Step : 346\n",
            "blue visited [11] state\n",
            "red_reward = [-19.], blue_reward = [19.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 347\n",
            "red visited [0] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 348\n",
            "blue visited [1] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 349\n",
            "red visited [42] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 350\n",
            "blue visited [11] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 351\n",
            "red visited [14] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 352\n",
            "blue visited [12] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 353\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 354\n",
            "blue visited [26] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 355\n",
            "red visited [30] state\n",
            "red_reward = [12.], blue_reward = [-12.]\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 356\n",
            "blue visited [28] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 357\n",
            "red visited [30] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 358\n",
            "blue visited [23] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "Step : 359\n",
            "red visited [49] state\n",
            "red_reward = 0, blue_reward = 0\n",
            "current winner = blue\n",
            "================================================================\n",
            "36 14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To do Double Deep Q-Network (DDQN)\n",
        "\n",
        "source: \n",
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "https://yjs-program.tistory.com/173\n",
        "\n",
        "https://towardsdatascience.com/double-deep-q-networks-905dd8325412\n",
        "\n",
        "https://github.com/chinancheng/DDQN.pytorch\n",
        "\n",
        "https://velog.io/@d2h10s/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-Double-DQN"
      ],
      "metadata": {
        "id": "GYR6TOgSYUF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "import gym\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "EPISODES = 50\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "GAMMA = 0.8\n",
        "\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n",
        "\n",
        "def conv2d_size_out(size, kernel_size, stride):\n",
        "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "\n",
        "import copy\n",
        "\n",
        "# Example class\n",
        "class MyClass:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MyClass(name='{self.name}')\"\n",
        "\n",
        "# Creating an instance of MyClass\n",
        "obj1 = MyClass(\"Object 1\")\n",
        "\n",
        "# Shallow copy\n",
        "obj2 = copy.copy(obj1)\n",
        "\n",
        "# Modifying obj2\n",
        "obj2.name = \"Object 2\"\n",
        "\n",
        "# Printing both objects\n",
        "print(obj1)  # Output: MyClass(name='Object 1')\n",
        "print(obj2)  # Output: MyClass(name='Object 2')\n",
        "\n",
        "# Example with a nested object\n",
        "nested_obj1 = [1, 2, [3, 4]]\n",
        "\n",
        "# Deep copy\n",
        "nested_obj2 = copy.deepcopy(nested_obj1)\n",
        "\n",
        "# Modifying nested_obj2\n",
        "nested_obj2[2][0] = 5\n",
        "\n",
        "'''\n",
        "# Printing both objects\n",
        "print(nested_obj1)  # Output: [1, 2, [3, 4]]\n",
        "print(nested_obj2)  # Output: [1, 2, [5, 4]]\n",
        "'''\n",
        "\n",
        "class redDDQNAgent:\n",
        "  def __init__(self):\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(153,100),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(100,51)\n",
        "    )\n",
        "    self.target = copy.deepcopy(self.model)\n",
        "    \n",
        "    self.optimizer = optim.Adam(self.model.parameters(),LR)\n",
        "    self.steps_done = 0\n",
        "    self.memory = deque(maxlen=50000)\n",
        "    self.reward_sum = 0.0\n",
        "\n",
        "  def memorize(self, situation, red_action, red_reward, blue_reward, next_situation):\n",
        "    situation=situation/800\n",
        "    situation=situation*[8, 1, 1]\n",
        "    situation=torch.Tensor(situation)\n",
        "    situation=situation.view(1,153)\n",
        "\n",
        "    red_reward = red_reward / 100\n",
        "    blue_reward = blue_reward / 100\n",
        "\n",
        "    red_action = torch.Tensor(red_action)\n",
        "    red_action = red_action.reshape(1)\n",
        "    \n",
        "    red_reward = torch.from_numpy(np.array(red_reward))\n",
        "    red_reward = red_reward.reshape(1)\n",
        "    blue_reward = torch.from_numpy(np.array(blue_reward))\n",
        "    blue_reward = blue_reward.reshape(1)\n",
        "\n",
        "    next_situation=next_situation/800\n",
        "    next_situation=next_situation*[8, 1, 1]\n",
        "    next_situation=torch.Tensor(next_situation)\n",
        "    next_situation=next_situation.view(1,153)\n",
        "\n",
        "    situation = situation.type(torch.FloatTensor)\n",
        "    red_action = red_action.type(torch.int64)\n",
        "    red_reward = red_reward.type(torch.FloatTensor)\n",
        "    blue_reward = blue_reward.type(torch.FloatTensor)\n",
        "    next_situation = next_situation.type(torch.FloatTensor)\n",
        "\n",
        "    self.memory.append((situation, red_action, red_reward, blue_reward, next_situation))\n",
        "\n",
        "\n",
        "  def learn(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    batch=random.sample(self.memory, BATCH_SIZE)\n",
        "    situations, red_actions, red_rewards, blue_rewards, next_situations = zip(*batch)\n",
        "\n",
        "    situations=torch.cat(situations)\n",
        "    red_actions=torch.cat(red_actions)\n",
        "    red_actions = red_actions.reshape(-1,1)\n",
        "    red_rewards=torch.cat(red_rewards)\n",
        "    blue_rewards=torch.cat(blue_rewards)\n",
        "    next_situations=torch.cat(next_situations)\n",
        "\n",
        "    current_q=self.model(situations).gather(1, red_actions)\n",
        "\n",
        "    max_next_q=self.target(next_situations).detach().max(1)[0]\n",
        "    expected_q=red_rewards+(GAMMA*max_next_q)\n",
        "\n",
        "    loss=F.mse_loss(current_q.squeeze(),expected_q)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def get_q_value(self, situation):\n",
        "    situation = situation / 800\n",
        "    situation = situation * [8, 1, 1]\n",
        "    situation = torch.Tensor(situation)\n",
        "    situation = situation.view(-1, 153)\n",
        "    q_values = self.model(situation)\n",
        "    return q_values\n",
        "\n",
        "  def get_action(self, situation, epsilon=0.1):\n",
        "    q_values = self.get_q_value(situation)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # 탐험(exploration)을 위해 랜덤한 행동 선택\n",
        "        #print(\"if executed: \")\n",
        "        action = random.randint(0, 50)\n",
        "        action = torch.from_numpy(np.array(action))\n",
        "\n",
        "    else:\n",
        "        # 탐색된 정보를 바탕으로 가장 높은 Q-value를 가진 행동 선택\n",
        "        action = q_values.argmax()\n",
        "        #print(\"else executed: \", action)\n",
        "  \n",
        "    return action\n",
        "\n",
        "\n",
        "  \n",
        "class blueDDQNAgent:\n",
        "  def __init__(self):\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(153,100),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(100,51)\n",
        "    )\n",
        "    self.target = copy.deepcopy(self.model)\n",
        "    \n",
        "    self.optimizer = optim.Adam(self.model.parameters(),LR)\n",
        "    self.steps_done = 0\n",
        "    self.memory = deque(maxlen=50000)\n",
        "    self.reward_sum = 0.0\n",
        "\n",
        "  def memorize(self, situation, blue_action, red_reward, blue_reward, next_situation):\n",
        "    situation=situation/800\n",
        "    situation=situation*[8, 1, 1]\n",
        "    situation=torch.Tensor(situation)\n",
        "    situation=situation.view(1,153)\n",
        "\n",
        "    red_reward = red_reward / 100\n",
        "    blue_reward = blue_reward / 100\n",
        "\n",
        "    blue_action = torch.Tensor(blue_action)\n",
        "    blue_action = blue_action.reshape(1)\n",
        "    \n",
        "    red_reward = torch.from_numpy(np.array(red_reward))\n",
        "    red_reward = red_reward.reshape(1)\n",
        "    blue_reward = torch.from_numpy(np.array(blue_reward))\n",
        "    blue_reward = blue_reward.reshape(1)\n",
        "\n",
        "    next_situation=next_situation/800\n",
        "    next_situation=next_situation*[8, 1, 1]\n",
        "    next_situation=torch.Tensor(next_situation)\n",
        "    next_situation=next_situation.view(1,153)\n",
        "\n",
        "    situation = situation.type(torch.FloatTensor)\n",
        "    blue_action = blue_action.type(torch.int64)\n",
        "    red_reward = red_reward.type(torch.FloatTensor)\n",
        "    blue_reward = blue_reward.type(torch.FloatTensor)\n",
        "    next_situation = next_situation.type(torch.FloatTensor)\n",
        "\n",
        "    self.memory.append((situation, blue_action, red_reward, blue_reward, next_situation))\n",
        "\n",
        "\n",
        "  def learn(self):\n",
        "    if len(self.memory) < BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    batch=random.sample(self.memory, BATCH_SIZE)\n",
        "    situations, blue_actions, red_rewards, blue_rewards, next_situations = zip(*batch)\n",
        "\n",
        "    situations=torch.cat(situations)\n",
        "    blue_actions=torch.cat(blue_actions)\n",
        "    blue_actions = blue_actions.reshape(-1,1)\n",
        "    red_rewards=torch.cat(red_rewards)\n",
        "    blue_rewards=torch.cat(blue_rewards)\n",
        "    next_situations=torch.cat(next_situations)\n",
        "\n",
        "    current_q=self.model(situations).gather(1, blue_actions)\n",
        "\n",
        "    max_next_q=self.target(next_situations).detach().max(1)[0]\n",
        "    expected_q=blue_rewards+(GAMMA*max_next_q)\n",
        "\n",
        "    loss=F.mse_loss(current_q.squeeze(),expected_q)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def get_q_value(self, situation):\n",
        "    situation = situation / 800\n",
        "    situation = situation * [8, 1, 1]\n",
        "    situation = torch.Tensor(situation)\n",
        "    situation = situation.view(-1, 153)\n",
        "    q_values = self.model(situation)\n",
        "    return q_values\n",
        "\n",
        "  def get_action(self, situation, epsilon=0.1):\n",
        "    q_values = self.get_q_value(situation)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # 탐험(exploration)을 위해 랜덤한 행동 선택\n",
        "        #print(\"if executed: \")\n",
        "        action = random.randint(0, 50)\n",
        "        action = torch.from_numpy(np.array(action))\n",
        "\n",
        "    else:\n",
        "        # 탐색된 정보를 바탕으로 가장 높은 Q-value를 가진 행동 선택\n",
        "        action = q_values.argmax()\n",
        "        #print(\"else executed: \", action)\n",
        "  \n",
        "    return action\n",
        "\n"
      ],
      "metadata": {
        "id": "k-bzpTRAuuu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def case_1():  \n",
        "  # Case.1: R: Q / B: Random\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDDQNAgent()\n",
        "  blue_agent = blueDDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DDQN vs Random\")\n",
        "  plt.legend(['Red: DDQN', 'Blue: Random'])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_2():\n",
        "  # Case.2: R: Q / B: Sub_optimal\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDDQNAgent()\n",
        "  blue_agent = blueDDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_blue()  \n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DDQN vs Sub_optimal\")\n",
        "  plt.legend(['Red: DDQN', 'Blue: Sub_optimal'])\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def case_3():\n",
        "  # Case.3: R: Random / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDDQNAgent()\n",
        "  blue_agent = blueDDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = env.action_space.sample()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"Random vs DDQN\")\n",
        "  plt.legend(['Red: Random', 'Blue: DDQN'])\n",
        "  plt.show()\n",
        "\n",
        "def case_4():\n",
        "  # Case.4: R: Sub_optimal / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDDQNAgent()\n",
        "  blue_agent = blueDDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      \n",
        "      red_action = SubOptimalActions(situation, env.standard_point).sub_optimal_action_red()\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "\n",
        "      #actions.set_state(situation)\n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel('# of episodes')\n",
        "  plt.ylabel('Sum of rewards')\n",
        "  plt.title('Sub_optimal vs DDQN')\n",
        "  plt.legend(['Red:Sub_optimal', 'Blue: DDQN'])\n",
        "  plt.show()\n",
        "\n",
        "def case_5():  \n",
        "  # Case.1: R: Q / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDDQNAgent()\n",
        "  blue_agent = blueDDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DDQN vs DDQN\")\n",
        "  plt.legend(['Red: DDQN', 'Blue: DDQN'])\n",
        "  plt.show()\n",
        "\n",
        "def case_6():  \n",
        "  # Case.1: R: Q / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDQNAgent()\n",
        "  blue_agent = blueDDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DQN vs DDQN\")\n",
        "  plt.legend(['Red: DQN', 'Blue: DDQN'])\n",
        "  plt.show()\n",
        "\n",
        "def case_7():  \n",
        "  # Case.1: R: Q / B: Q\n",
        "  env = US_vote(Initial_state,180)\n",
        "  red_agent = redDDQNAgent()\n",
        "  blue_agent = blueDQNAgent()\n",
        "  score_history = []\n",
        "  red_reward_recorder = []\n",
        "  blue_reward_recorder = []\n",
        "  for e in range(1,EPISODES+1):\n",
        "    situation=env.reset()\n",
        "    steps=0\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      red_action = red_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"red\",red_action)\n",
        "\n",
        "      red_agent.memorize(old_situation,red_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      red_agent.learn()\n",
        "      \n",
        "      env.render()\n",
        "      blue_action = blue_agent.get_action(situation)\n",
        "      old_situation=situation\n",
        "      situation, red_reward, blue_reward, done, info = env.step(\"blue\",blue_action)\n",
        "\n",
        "      blue_agent.memorize(old_situation,blue_action,red_reward,blue_reward,situation)\n",
        "      red_agent.reward_sum += red_reward\n",
        "      blue_agent.reward_sum += blue_reward\n",
        "      blue_agent.learn()\n",
        "\n",
        "      steps+=1\n",
        "\n",
        "      if done:\n",
        "        score_history.append(steps)\n",
        "        break\n",
        "    red_reward_recorder.append(red_agent.reward_sum)\n",
        "    blue_reward_recorder.append(blue_agent.reward_sum)\n",
        "    red_agent.reward_sum = 0\n",
        "    blue_agent.reward_sum = 0\n",
        "\n",
        "  print(len(red_reward_recorder))\n",
        "  print(len(blue_reward_recorder))\n",
        "  plt.plot(red_reward_recorder)\n",
        "  plt.plot(blue_reward_recorder)\n",
        "  plt.xlabel(\"# of Episodes\")\n",
        "  plt.ylabel(\"Sum of Reward\")\n",
        "  plt.title(\"DDQN vs DQN\")\n",
        "  plt.legend(['Red: DDQN', 'Blue: DQN'])\n",
        "  plt.show()\n",
        "\n",
        "#case_1()\n",
        "#case_2()\n",
        "#case_3()\n",
        "#case_4()\n",
        "#case_5()\n",
        "#case_6()\n",
        "#case_7()"
      ],
      "metadata": {
        "id": "vImqBpMcuyd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}